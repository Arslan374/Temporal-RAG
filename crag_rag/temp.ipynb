{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d71492",
   "metadata": {},
   "source": [
    "2. Data Loading & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1130992",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Assume you have your datasets (TAQA, TempQuestions, TimeQA, CRAG) downloaded\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# and in a structured format (e.g., JSON files).\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Assume you have your datasets (TAQA, TempQuestions, TimeQA, CRAG) downloaded\n",
    "# and in a structured format (e.g., JSON files).\n",
    "\n",
    "def load_json_dataset(filepath: str) -> List[Dict]:\n",
    "    \"\"\"Loads a dataset from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocess_document(doc: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Basic preprocessing for a document.\n",
    "    Assumes doc has 'id', 'text', and 'timestamp' (optional).\n",
    "    \"\"\"\n",
    "    doc_id = doc.get('id')\n",
    "    text = doc.get('text', '').strip()\n",
    "    timestamp = doc.get('timestamp') # Store as string, parse later if needed\n",
    "    \n",
    "    # Add more cleaning/tokenization as needed\n",
    "    \n",
    "    return {'id': doc_id, 'text': text, 'timestamp': timestamp}\n",
    "\n",
    "def preprocess_question(question: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Basic preprocessing for a question.\n",
    "    Assumes question has 'id', 'text', and 'ground_truth_temporal' (for Time-Aware Module testing).\n",
    "    \"\"\"\n",
    "    q_id = question.get('id')\n",
    "    text = question.get('question_text', '').strip()\n",
    "    # This 'ground_truth_temporal' would be crucial for evaluating your TimeAwareModule\n",
    "    ground_truth_temporal = question.get('is_temporal', None) \n",
    "    \n",
    "    return {'id': q_id, 'text': text, 'ground_truth_temporal': ground_truth_temporal}\n",
    "\n",
    "\n",
    "# Example usage (assuming dummy files exist)\n",
    "# documents_data = load_json_dataset('path/to/your/documents.json')\n",
    "# questions_data = load_json_dataset('path/to/your/questions.json')\n",
    "\n",
    "# processed_documents = [preprocess_document(d) for d in documents_data]\n",
    "# processed_questions = [preprocess_question(q) for q in questions_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e0cd1",
   "metadata": {},
   "source": [
    "3. Modular Component: Document Encoder & Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4275d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class DocumentEncoder:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initializes the document encoder using a SentenceTransformer model.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        print(f\"DocumentEncoder loaded model {model_name} on {self.device}\")\n",
    "\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a list of texts into embeddings.\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "        return embeddings\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes a FAISS index for document embeddings.\n",
    "        \"\"\"\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim) # Using L2 distance for similarity search\n",
    "        self.doc_ids = []\n",
    "        self.metadata = []\n",
    "        print(f\"VectorStore initialized with embedding dimension: {embedding_dim}\")\n",
    "\n",
    "    def add_documents(self, embeddings: np.ndarray, doc_ids: List[str], metadata: List[Dict]):\n",
    "        \"\"\"\n",
    "        Adds document embeddings, IDs, and metadata to the FAISS index.\n",
    "        \"\"\"\n",
    "        if embeddings.shape[0] != len(doc_ids) or embeddings.shape[0] != len(metadata):\n",
    "            raise ValueError(\"Mismatched dimensions for embeddings, doc_ids, and metadata.\")\n",
    "        self.index.add(embeddings)\n",
    "        self.doc_ids.extend(doc_ids)\n",
    "        self.metadata.extend(metadata)\n",
    "        print(f\"Added {embeddings.shape[0]} documents to VectorStore.\")\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[str, float, Dict]]:\n",
    "        \"\"\"\n",
    "        Searches the vector store for the top-k nearest neighbors.\n",
    "        Returns a list of tuples: (doc_id, similarity_score, metadata).\n",
    "        \"\"\"\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1) # Ensure 2D for FAISS search\n",
    "\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for i, dist in zip(indices[0], distances[0]):\n",
    "            if i == -1: # No more results\n",
    "                continue\n",
    "            doc_id = self.doc_ids[i]\n",
    "            doc_meta = self.metadata[i]\n",
    "            # FAISS returns L2 distance, convert to similarity if needed (e.g., 1 - normalized_l2_distance)\n",
    "            # For simplicity, we'll return the raw distance for now, smaller is better.\n",
    "            results.append((doc_id, dist, doc_meta)) \n",
    "        return results\n",
    "\n",
    "    def save_index(self, path: str):\n",
    "        faiss.write_index(self.index, path)\n",
    "        # You'll also need to save doc_ids and metadata separately\n",
    "        with open(f\"{path}.doc_ids.json\", 'w') as f:\n",
    "            json.dump(self.doc_ids, f)\n",
    "        with open(f\"{path}.metadata.json\", 'w') as f:\n",
    "            json.dump(self.metadata, f)\n",
    "        print(f\"VectorStore index saved to {path}\")\n",
    "\n",
    "    def load_index(self, path: str):\n",
    "        self.index = faiss.read_index(path)\n",
    "        with open(f\"{path}.doc_ids.json\", 'r') as f:\n",
    "            self.doc_ids = json.load(f)\n",
    "        with open(f\"{path}.metadata.json\", 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        print(f\"VectorStore index loaded from {path}\")\n",
    "\n",
    "\n",
    "# --- Script to build initial document index (conceptual) ---\n",
    "# This script would be run once to prepare your document embeddings\n",
    "def build_document_index(documents: List[Dict], output_dir: str = 'index_data'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    doc_encoder = DocumentEncoder()\n",
    "    texts = [doc['text'] for doc in documents]\n",
    "    doc_ids = [doc['id'] for doc in documents]\n",
    "    metadata = [{'timestamp': doc['timestamp'], 'text': doc['text']} for doc in documents] # Store full text for retrieval later\n",
    "\n",
    "    embeddings = doc_encoder.encode(texts)\n",
    "    \n",
    "    # Assuming all-MiniLM-L6-v2 produces 384-dim embeddings\n",
    "    vector_store = VectorStore(embedding_dim=embeddings.shape[1]) \n",
    "    vector_store.add_documents(embeddings, doc_ids, metadata)\n",
    "    \n",
    "    index_path = os.path.join(output_dir, 'document_index.faiss')\n",
    "    vector_store.save_index(index_path)\n",
    "    print(\"Document index built and saved.\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'processed_documents' from Phase 2\n",
    "# build_document_index(processed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0eff2",
   "metadata": {},
   "source": [
    "4. Modular Component: Time-Aware Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class TimeAwareModule:\n",
    "    def __init__(self):\n",
    "        # Basic regex patterns for temporal cues. Expand this significantly!\n",
    "        # This is a placeholder; a fine-tuned classifier would be more robust.\n",
    "        self.temporal_keywords = [\n",
    "            r'\\bwhen\\b', r'\\bafter\\b', r'\\bbefore\\b', r'\\bduring\\b', r'\\bin\\s+\\d{4}\\b',\n",
    "            r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{4}\\b',\n",
    "            r'\\d{2}/\\d{2}/\\d{4}', r'\\d{4}-\\d{2}-\\d{2}', r'\\d{4}s\\b' # 2000s\n",
    "        ]\n",
    "        self.keyword_patterns = [re.compile(p, re.IGNORECASE) for p in self.temporal_keywords]\n",
    "        print(\"TimeAwareModule initialized with basic temporal keyword patterns.\")\n",
    "\n",
    "    def get_temporal_score(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a simple temporal score based on keyword presence.\n",
    "        A more sophisticated model would use a trained classifier.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        for pattern in self.keyword_patterns:\n",
    "            if pattern.search(text):\n",
    "                score += 1 # Increment for each pattern found\n",
    "        return score / len(self.keyword_patterns) # Normalize between 0 and 1\n",
    "\n",
    "    def is_temporal_query(self, query_text: str, threshold: float = 0.1) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a query is temporal based on its temporal score.\n",
    "        \"\"\"\n",
    "        score = self.get_temporal_score(query_text)\n",
    "        return score > threshold\n",
    "\n",
    "    def get_temporal_relevance_from_timestamps(self, query_timestamp_str: str, doc_timestamp_str: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculates temporal relevance based on the time difference between query and document timestamps.\n",
    "        Inspired by TempRALM. Assumes YYYY-MM-DD or YYYY-MM format.\n",
    "        Returns a score where 1.0 is perfect match, decreasing with difference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Attempt to parse as full date first, then as year-month\n",
    "            query_time = self._parse_timestamp(query_timestamp_str)\n",
    "            doc_time = self._parse_timestamp(doc_timestamp_str)\n",
    "\n",
    "            if query_time is None or doc_time is None:\n",
    "                return 0.0 # Cannot parse timestamps, no temporal relevance from this method\n",
    "\n",
    "            time_diff = abs((query_time - doc_time).days) # Difference in days\n",
    "            # You'll need to define a decay function. This is a simple inverse linear decay.\n",
    "            # Adjust the 'scale_factor' based on your data and desired sensitivity.\n",
    "            scale_factor = 365 * 5 # E.g., significant decay after 5 years\n",
    "            relevance = max(0.0, 1.0 - (time_diff / scale_factor))\n",
    "            return relevance\n",
    "        except Exception as e:\n",
    "            # print(f\"Error parsing timestamps for temporal relevance: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _parse_timestamp(self, ts_str: str):\n",
    "        if not ts_str:\n",
    "            return None\n",
    "        try:\n",
    "            return datetime.strptime(ts_str, '%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return datetime.strptime(ts_str, '%Y-%m') # For year-month only\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    return datetime.strptime(ts_str, '%Y') # For year only\n",
    "                except ValueError:\n",
    "                    return None # Unable to parse\n",
    "\n",
    "# --- Standalone Test Script for Time-Aware Module ---\n",
    "# Save this in a separate file like test_time_aware_module.py\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    module = TimeAwareModule()\n",
    "\n",
    "    # Test get_temporal_score\n",
    "    print(\"\\n--- Testing Temporal Score ---\")\n",
    "    print(f\"'What happened in 2008?' Score: {module.get_temporal_score('What happened in 2008?')}\")\n",
    "    print(f\"'Who is the president?' Score: {module.get_temporal_score('Who is the president?')}\")\n",
    "    print(f\"'Events after March 2020?' Score: {module.get_temporal_score('Events after March 2020?')}\")\n",
    "    \n",
    "    # Test is_temporal_query\n",
    "    print(\"\\n--- Testing Temporal Query Classification ---\")\n",
    "    print(f\"'What happened in 2008?' Is temporal: {module.is_temporal_query('What happened in 2008?')}\")\n",
    "    print(f\"'Who is the president?' Is temporal: {module.is_temporal_query('Who is the president?')}\")\n",
    "    print(f\"'Events after March 2020?' Is temporal: {module.is_temporal_query('Events after March 2020?', threshold=0.2)}\")\n",
    "\n",
    "    # Test get_temporal_relevance_from_timestamps\n",
    "    print(\"\\n--- Testing Timestamp Relevance ---\")\n",
    "    print(f\"Query 2020-01-15, Doc 2020-01-20: {module.get_temporal_relevance_from_timestamps('2020-01-15', '2020-01-20')}\")\n",
    "    print(f\"Query 2020-01-15, Doc 2021-01-15: {module.get_temporal_relevance_from_timestamps('2020-01-15', '2021-01-15')}\")\n",
    "    print(f\"Query 2010, Doc 2020: {module.get_temporal_relevance_from_timestamps('2010', '2020')}\")\n",
    "    print(f\"Query no-date, Doc 2020: {module.get_temporal_relevance_from_timestamps('', '2020')}\")\n",
    "\n",
    "    # For robust evaluation, you'd load a test dataset with ground truth temporal labels\n",
    "    # Example (conceptual):\n",
    "    # test_questions = load_json_dataset('path/to/temporal_test_questions.json')\n",
    "    # correct_predictions = 0\n",
    "    # for q_data in test_questions:\n",
    "    #     question_text = q_data['text']\n",
    "    #     ground_truth = q_data['ground_truth_temporal'] # True/False\n",
    "    #     prediction = module.is_temporal_query(question_text)\n",
    "    #     if prediction == ground_truth:\n",
    "    #         correct_predictions += 1\n",
    "    # accuracy = correct_predictions / len(test_questions)\n",
    "    # print(f\"\\nTimeAwareModule Classification Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def463b9",
   "metadata": {},
   "source": [
    "5. Modular Component: Query Encoders & Query Router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3cb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer # For Contriever or other HuggingFace models\n",
    "\n",
    "class NormalQueryEncoder:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        print(f\"NormalQueryEncoder loaded model {model_name} on {self.device}\")\n",
    "\n",
    "    def encode(self, query_text: str) -> np.ndarray:\n",
    "        embedding = self.model.encode(query_text, convert_to_numpy=True)\n",
    "        return embedding\n",
    "\n",
    "class TemporalQueryEncoder:\n",
    "    def __init__(self, model_path: str = 'path/to/your/fine_tuned_contriever.bin'):\n",
    "        \"\"\"\n",
    "        Initializes the temporal query encoder using your fine-tuned Contriever model.\n",
    "        Assumes model_path points to the PyTorch .bin file for Contriever weights.\n",
    "        You might need to adjust this based on the exact Contriever implementation.\n",
    "        \"\"\"\n",
    "        # Load Contriever model (assuming it's a HuggingFace-compatible format or requires custom loading)\n",
    "        # Placeholder: This might need adjustment based on how 'path/to/your/fine_tuned_contriever.bin' is structured.\n",
    "        # Often, fine-tuned models can be loaded via AutoModel.from_pretrained if they have a config.json.\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('facebook/contriever') # Or relevant Contriever tokenizer\n",
    "            self.model = AutoModel.from_pretrained('facebook/contriever') # Load base Contriever architecture\n",
    "            # Then load the state_dict from your .bin file\n",
    "            state_dict = torch.load(model_path, map_location=torch.device('cpu')) # Load to CPU first\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval() # Set to evaluation mode\n",
    "            print(f\"TemporalQueryEncoder loaded fine-tuned Contriever from {model_path} on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fine-tuned Contriever model: {e}\")\n",
    "            print(\"Please ensure 'model_path' is correct and compatible with AutoModel.from_pretrained or provide custom loading logic.\")\n",
    "            self.model = None # Indicate failure to load\n",
    "\n",
    "    def encode(self, query_text: str) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"TemporalQueryEncoder model not loaded.\")\n",
    "        inputs = self.tokenizer(query_text, return_tensors='pt', padding=True, truncation=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Contriever typically uses the mean of the last hidden state as embedding\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "        return embedding\n",
    "\n",
    "class QueryRouter:\n",
    "    def __init__(self, time_aware_module: TimeAwareModule, \n",
    "                 normal_encoder: NormalQueryEncoder, \n",
    "                 temporal_encoder: TemporalQueryEncoder,\n",
    "                 temporal_threshold: float = 0.1):\n",
    "        self.time_aware_module = time_aware_module\n",
    "        self.normal_encoder = normal_encoder\n",
    "        self.temporal_encoder = temporal_encoder\n",
    "        self.temporal_threshold = temporal_threshold\n",
    "        print(\"QueryRouter initialized.\")\n",
    "\n",
    "    def route_and_encode(self, query_text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Routes the query to the appropriate encoder based on temporal analysis.\n",
    "        Returns the encoded query embedding.\n",
    "        \"\"\"\n",
    "        if self.time_aware_module.is_temporal_query(query_text, self.temporal_threshold):\n",
    "            print(f\"Query '{query_text}' identified as temporal. Using TemporalQueryEncoder.\")\n",
    "            return self.temporal_encoder.encode(query_text)\n",
    "        else:\n",
    "            print(f\"Query '{query_text}' identified as non-temporal. Using NormalQueryEncoder.\")\n",
    "            return self.normal_encoder.encode(query_text)\n",
    "\n",
    "# Example usage (conceptual):\n",
    "# time_aware = TimeAwareModule()\n",
    "# normal_enc = NormalQueryEncoder()\n",
    "# temporal_enc = TemporalQueryEncoder(model_path='path/to/your/fine_tuned_contriever.bin') # REPLACE WITH ACTUAL PATH\n",
    "# router = QueryRouter(time_aware, normal_enc, temporal_enc)\n",
    "\n",
    "# query_embedding_temporal = router.route_and_encode(\"What happened in the stock market after 2008?\")\n",
    "# query_embedding_normal = router.route_and_encode(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8afb87",
   "metadata": {},
   "source": [
    "6. Modular Component: Large Language Model (LLM) Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class LLMGenerator:\n",
    "    def __init__(self, model_name: str = \"distilbert/distilgpt2\", device: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the LLM for answer generation.\n",
    "        You can replace \"distilbert/distilgpt2\" with other models like 'meta-llama/Llama-2-7b-chat-hf',\n",
    "        'google/flan-t5-large', etc.\n",
    "        Note: Larger models require more resources.\n",
    "        \"\"\"\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        try:\n",
    "            # Using a text-generation pipeline for simplicity. Adjust for chat models.\n",
    "            self.generator = pipeline(\"text-generation\", model=model_name, device=self.device)\n",
    "            print(f\"LLMGenerator loaded model {model_name} on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading LLM model {model_name}: {e}\")\n",
    "            print(\"Please ensure the model name is correct and you have enough resources.\")\n",
    "            self.generator = None\n",
    "\n",
    "    def generate_answer(self, question: str, context: List[str], max_length: int = 200) -> str:\n",
    "        \"\"\"\n",
    "        Generates an answer based on the question and provided context.\n",
    "        \"\"\"\n",
    "        if self.generator is None:\n",
    "            return \"Error: LLM model not loaded.\"\n",
    "\n",
    "        # Combine question and context into a prompt.\n",
    "        # This prompt format is crucial for RAG performance and can be fine-tuned.\n",
    "        context_str = \"\\n\".join(context)\n",
    "        prompt = f\"Context: {context_str}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "        # Generate text. You might need to adjust generation parameters.\n",
    "        try:\n",
    "            response = self.generator(prompt, max_new_tokens=max_length, do_sample=False,\n",
    "                                      num_return_sequences=1, return_full_text=False)\n",
    "            if response and len(response) > 0:\n",
    "                answer = response[0]['generated_text'].strip()\n",
    "                # Basic post-processing to remove potential prompt repetition or incomplete sentences\n",
    "                if \"Question:\" in answer:\n",
    "                    answer = answer.split(\"Question:\")[0].strip()\n",
    "                if \"Answer:\" in answer: # In case the model generates \"Answer:\" itself\n",
    "                    answer = answer.split(\"Answer:\", 1)[-1].strip()\n",
    "                return answer\n",
    "            return \"No answer generated.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error during answer generation: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9597bd8",
   "metadata": {},
   "source": [
    "7. Simple RAG Baseline Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    def __init__(self, document_encoder: DocumentEncoder, vector_store: VectorStore, llm_generator: LLMGenerator):\n",
    "        \"\"\"\n",
    "        Initializes the Simple RAG system.\n",
    "        \"\"\"\n",
    "        self.document_encoder = document_encoder\n",
    "        self.vector_store = vector_store\n",
    "        self.llm_generator = llm_generator\n",
    "        print(\"SimpleRAG system initialized.\")\n",
    "\n",
    "    def answer_question(self, query_text: str, k: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Answers a question using a simple RAG approach.\n",
    "        1. Encodes the query.\n",
    "        2. Retrieves top-k documents.\n",
    "        3. Generates an answer using the LLM based on retrieved context.\n",
    "        \"\"\"\n",
    "        # 1. Encode the query\n",
    "        query_embedding = self.document_encoder.encode([query_text])[0] # [0] because encode returns a batch\n",
    "\n",
    "        # 2. Retrieve top-k documents\n",
    "        retrieved_results = self.vector_store.search(query_embedding, k=k)\n",
    "        \n",
    "        # Extract relevant content from retrieved documents\n",
    "        # Note: 'text' was stored in metadata during index building\n",
    "        contexts = [result[2]['text'] for result in retrieved_results] # result[2] is metadata dict\n",
    "\n",
    "        if not contexts:\n",
    "            return \"No relevant documents found.\"\n",
    "\n",
    "        # 3. Generate answer using LLM\n",
    "        answer = self.llm_generator.generate_answer(question=query_text, context=contexts)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4f3ba",
   "metadata": {},
   "source": [
    "8. Full Proposed Pipeline Implementation (TemporalRAGPipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce35c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class TemporalRAGPipeline:\n",
    "    def __init__(self, time_aware_module: TimeAwareModule, \n",
    "                 query_router: QueryRouter, \n",
    "                 vector_store: VectorStore, \n",
    "                 llm_generator: LLMGenerator,\n",
    "                 re_ranking_weights: Dict[str, float] = None):\n",
    "        \"\"\"\n",
    "        Initializes the full Temporal RAG Pipeline.\n",
    "        re_ranking_weights: Dictionary for combining scores, e.g., {'semantic': 0.5, 'temporal_metadata': 0.3, 'temporal_content': 0.2}\n",
    "        \"\"\"\n",
    "        self.time_aware_module = time_aware_module\n",
    "        self.query_router = query_router\n",
    "        self.vector_store = vector_store\n",
    "        self.llm_generator = llm_generator\n",
    "        self.re_ranking_weights = re_ranking_weights if re_ranking_weights else {\n",
    "            'semantic': 0.5,\n",
    "            'temporal_metadata': 0.3,\n",
    "            'temporal_content': 0.2\n",
    "        }\n",
    "        # Normalize weights to sum to 1.0\n",
    "        total_weight = sum(self.re_ranking_weights.values())\n",
    "        if total_weight > 0:\n",
    "            self.re_ranking_weights = {k: v / total_weight for k, v in self.re_ranking_weights.items()}\n",
    "        print(f\"TemporalRAGPipeline initialized with re-ranking weights: {self.re_ranking_weights}\")\n",
    "\n",
    "    def answer_question(self, query_text: str, k_retrieve: int = 10, k_rerank: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Answers a question using the full Temporal RAG pipeline.\n",
    "        1. Routes and encodes the query.\n",
    "        2. Retrieves top-k_retrieve documents.\n",
    "        3. Re-ranks documents using semantic, temporal (metadata), and temporal (content) scores.\n",
    "        4. Selects top-k_rerank documents after re-ranking.\n",
    "        5. Generates an answer using the LLM based on re-ranked context.\n",
    "        \"\"\"\n",
    "        # 1. Route and encode the query\n",
    "        query_embedding = self.query_router.route_and_encode(query_text)\n",
    "\n",
    "        # 2. Retrieve top-k_retrieve documents\n",
    "        # search returns: (doc_id, distance_score_L2, metadata_dict)\n",
    "        retrieved_results = self.vector_store.search(query_embedding, k=k_retrieve)\n",
    "\n",
    "        if not retrieved_results:\n",
    "            return \"No documents retrieved for re-ranking.\"\n",
    "\n",
    "        # 3. Re-rank documents\n",
    "        scored_documents = []\n",
    "        for doc_id, l2_distance, metadata in retrieved_results:\n",
    "            doc_text = metadata['text']\n",
    "            doc_timestamp = metadata['timestamp']\n",
    "\n",
    "            # Calculate Semantic Score (convert L2 distance to similarity, e.g., 1 / (1 + dist))\n",
    "            # Lower L2 distance means higher similarity. Max L2 could be large.\n",
    "            # A common way for cosine similarity (if embeddings are normalized) is (1 - L2_dist^2/2).\n",
    "            # For simplicity, let's use a normalized inverse:\n",
    "            semantic_score = 1.0 / (1.0 + l2_distance) if l2_distance >= 0 else 0.0 # Ensure non-negative distance\n",
    "\n",
    "            # Calculate Temporal Score (Metadata-based) - requires query timestamp if available\n",
    "            # For now, let's assume query_text contains a parsable timestamp if it's a temporal query.\n",
    "            # In a real system, you might need a separate component to extract query timestamps.\n",
    "            query_timestamp = None\n",
    "            if self.time_aware_module.is_temporal_query(query_text):\n",
    "                # Attempt to extract a year from the query for simple timestamp matching\n",
    "                year_match = re.search(r'\\b\\d{4}\\b', query_text)\n",
    "                if year_match:\n",
    "                    query_timestamp = year_match.group(0) # e.g., \"2008\"\n",
    "            \n",
    "            temporal_metadata_score = self.time_aware_module.get_temporal_relevance_from_timestamps(\n",
    "                query_timestamp, doc_timestamp\n",
    "            )\n",
    "\n",
    "            # Calculate Temporal Score (Content-based)\n",
    "            temporal_content_score = self.time_aware_module.get_temporal_score(doc_text)\n",
    "            \n",
    "            # Combine scores\n",
    "            final_re_ranking_score = (\n",
    "                self.re_ranking_weights.get('semantic', 0) * semantic_score +\n",
    "                self.re_ranking_weights.get('temporal_metadata', 0) * temporal_metadata_score +\n",
    "                self.re_ranking_weights.get('temporal_content', 0) * temporal_content_score\n",
    "            )\n",
    "            \n",
    "            scored_documents.append({'id': doc_id, 'text': doc_text, 'score': final_re_ranking_score})\n",
    "        \n",
    "        # Sort by final re-ranking score (highest score first)\n",
    "        scored_documents.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Select top-k_rerank documents\n",
    "        top_reranked_contexts = [doc['text'] for doc in scored_documents[:k_rerank]]\n",
    "\n",
    "        if not top_reranked_contexts:\n",
    "            return \"No documents left after re-ranking for answer generation.\"\n",
    "\n",
    "        # 4. Generate answer using LLM\n",
    "        answer = self.llm_generator.generate_answer(question=query_text, context=top_reranked_contexts)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d399649",
   "metadata": {},
   "source": [
    "9. Evaluation Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from typing import Callable\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        print(\"Evaluator initialized.\")\n",
    "\n",
    "    def calculate_metrics(self, predictions: List[str], ground_truths: List[str], questions: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculates various evaluation metrics.\n",
    "        This is a conceptual implementation. For real RAG evaluation, you'd use\n",
    "        more sophisticated metrics and possibly human judgment or specialized tools.\n",
    "        \n",
    "        For simplicity, let's assume ground_truths are direct answers and we can\n",
    "        do basic string matching for initial 'correctness'.\n",
    "        For hallucination/missing answers, you'd need more advanced NLP techniques\n",
    "        or human annotation.\n",
    "        \n",
    "        questions: List of original question dictionaries, including 'ground_truth_temporal' if used.\n",
    "        \"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        correct_answers = 0\n",
    "        for pred, gt in zip(predictions, ground_truths):\n",
    "            # Simple exact match (highly stringent)\n",
    "            if pred.strip().lower() == gt.strip().lower():\n",
    "                correct_answers += 1\n",
    "        \n",
    "        results['exact_match_accuracy'] = correct_answers / len(predictions) if predictions else 0\n",
    "\n",
    "        # Placeholder for more complex metrics (e.g., using an LLM to judge correctness,\n",
    "        # or measuring hallucination/missing info).\n",
    "        # For CRAG, you'd integrate their specific evaluation script.\n",
    "        # results['hallucination_rate'] = ...\n",
    "        # results['missing_answer_rate'] = ...\n",
    "        \n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        for metric, value in results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def evaluate_temporal_classification(self, time_aware_module: TimeAwareModule, temporal_test_questions: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the TimeAwareModule in classifying temporal queries.\n",
    "        temporal_test_questions: List of question dicts with 'text' and 'ground_truth_temporal' (True/False).\n",
    "        \"\"\"\n",
    "        if not temporal_test_questions:\n",
    "            print(\"No temporal test questions provided for TimeAwareModule evaluation.\")\n",
    "            return {}\n",
    "\n",
    "        true_labels = [q['ground_truth_temporal'] for q in temporal_test_questions if q['ground_truth_temporal'] is not None]\n",
    "        if not true_labels:\n",
    "            print(\"No ground truth temporal labels found in the test questions.\")\n",
    "            return {}\n",
    "\n",
    "        predictions = [time_aware_module.is_temporal_query(q['text']) for q in temporal_test_questions if q['ground_truth_temporal'] is not None]\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(true_labels, predictions),\n",
    "            'precision': precision_score(true_labels, predictions, zero_division=0),\n",
    "            'recall': recall_score(true_labels, predictions, zero_division=0),\n",
    "            'f1_score': f1_score(true_labels, predictions, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n--- TimeAwareModule Temporal Classification Results ---\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def run_evaluation_suite(self, \n",
    "                             model_or_pipeline_instance: Union[SimpleRAG, TemporalRAGPipeline], \n",
    "                             test_dataset: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Runs the full evaluation for a given model/pipeline on a test dataset.\n",
    "        test_dataset: List of question dictionaries, each with 'text' and 'answer' (ground truth).\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Running Evaluation for: {model_or_pipeline_instance.__class__.__name__} ---\")\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        for i, q_data in enumerate(test_dataset):\n",
    "            question_text = q_data['text']\n",
    "            ground_truth_answer = q_data['answer'] # Assuming 'answer' key for ground truth\n",
    "            \n",
    "            print(f\"Processing question {i+1}/{len(test_dataset)}: {question_text[:50]}...\")\n",
    "            predicted_answer = model_or_pipeline_instance.answer_question(question_text)\n",
    "            \n",
    "            predictions.append(predicted_answer)\n",
    "            ground_truths.append(ground_truth_answer)\n",
    "            # print(f\"Pred: {predicted_answer}\\nGT: {ground_truth_answer}\\n\") # Uncomment for debugging\n",
    "\n",
    "        results = self.calculate_metrics(predictions, ground_truths, test_dataset)\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
